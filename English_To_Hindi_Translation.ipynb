{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A75tcHbWxYRb",
        "outputId": "150d529e-4e31-45e9-f261-624e9f44ecbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 14s 14s/step - loss: 2.6827 - val_loss: 2.2141\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 2.6626 - val_loss: 2.2058\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 2.6409 - val_loss: 2.1961\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 2.6151 - val_loss: 2.1839\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 2.5821 - val_loss: 2.1686\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 2.5376 - val_loss: 2.1491\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 2.4749 - val_loss: 2.1245\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 2.3854 - val_loss: 2.0933\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 2.2673 - val_loss: 2.0640\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 2.1869 - val_loss: 2.0609\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 2.1589 - val_loss: 2.0657\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 2.1523 - val_loss: 2.0694\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 2.1465 - val_loss: 2.0685\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 2.1203 - val_loss: 2.0626\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 2.0686 - val_loss: 2.0522\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 1.9911 - val_loss: 2.0376\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 1.8904 - val_loss: 2.0197\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 1.7743 - val_loss: 2.0007\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 1.6571 - val_loss: 1.9830\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 1.5525 - val_loss: 1.9682\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 1.4638 - val_loss: 1.9561\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 1.3828 - val_loss: 1.9472\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 1.3149 - val_loss: 1.9425\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 1.2605 - val_loss: 1.9411\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 1.2117 - val_loss: 1.9408\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 1.1663 - val_loss: 1.9404\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 1.1129 - val_loss: 1.9403\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 1.0407 - val_loss: 1.9407\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.9518 - val_loss: 1.9406\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.8657 - val_loss: 1.9375\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.7910 - val_loss: 1.9323\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.7171 - val_loss: 1.9289\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.6660 - val_loss: 1.9304\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.6144 - val_loss: 1.9354\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.5557 - val_loss: 1.9398\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.5095 - val_loss: 1.9420\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.4647 - val_loss: 1.9435\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.4300 - val_loss: 1.9471\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.3846 - val_loss: 1.9520\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.3467 - val_loss: 1.9564\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.3176 - val_loss: 1.9620\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.2926 - val_loss: 1.9702\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.2650 - val_loss: 1.9783\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.2424 - val_loss: 1.9834\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.2280 - val_loss: 1.9863\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.2119 - val_loss: 1.9894\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.2001 - val_loss: 1.9937\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.1874 - val_loss: 1.9974\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.1778 - val_loss: 1.9996\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.1656 - val_loss: 2.0008\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.1572 - val_loss: 2.0026\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.1500 - val_loss: 2.0060\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.1416 - val_loss: 2.0102\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1317 - val_loss: 2.0144\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.1257 - val_loss: 2.0189\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.1230 - val_loss: 2.0245\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.1185 - val_loss: 2.0314\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.1114 - val_loss: 2.0391\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.1056 - val_loss: 2.0469\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.1018 - val_loss: 2.0554\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.0972 - val_loss: 2.0652\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.0922 - val_loss: 2.0765\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0881 - val_loss: 2.0891\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.0847 - val_loss: 2.1024\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0797 - val_loss: 2.1154\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.0754 - val_loss: 2.1279\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0734 - val_loss: 2.1406\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0716 - val_loss: 2.1543\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.0671 - val_loss: 2.1686\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0635 - val_loss: 2.1829\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.0616 - val_loss: 2.1971\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.0590 - val_loss: 2.2110\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0562 - val_loss: 2.2242\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.0543 - val_loss: 2.2368\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.0526 - val_loss: 2.2484\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.0493 - val_loss: 2.2587\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0471 - val_loss: 2.2678\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.0461 - val_loss: 2.2762\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0447 - val_loss: 2.2841\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.0427 - val_loss: 2.2912\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.0420 - val_loss: 2.2977\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.0416 - val_loss: 2.3037\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.0399 - val_loss: 2.3089\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0394 - val_loss: 2.3136\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.0391 - val_loss: 2.3180\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.0376 - val_loss: 2.3220\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0371 - val_loss: 2.3257\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.0373 - val_loss: 2.3294\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.0367 - val_loss: 2.3329\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0369 - val_loss: 2.3359\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0377 - val_loss: 2.3389\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.0371 - val_loss: 2.3418\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0367 - val_loss: 2.3447\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.0368 - val_loss: 2.3476\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.0362 - val_loss: 2.3504\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.0358 - val_loss: 2.3528\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.0362 - val_loss: 2.3550\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.0360 - val_loss: 2.3572\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.0356 - val_loss: 2.3594\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.0356 - val_loss: 2.3620\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Input: Thank you\n",
            "Translated: ठीक हूँ,\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Example corpus with '<start>' and '<end>' tokens\n",
        "corpus = [\n",
        "    (\"Hello\", \"<start> नमस्ते <end>\"),\n",
        "    (\"How are you?\", \"<start> आप कैसे हैं? <end>\"),\n",
        "    (\"I am fine, thank you.\", \"<start> मैं ठीक हूँ, धन्यवाद। <end>\"),\n",
        "    (\"What is your name?\", \"<start> आपका नाम क्या है? <end>\"),\n",
        "    (\"My name is John.\", \"<start> मेरा नाम जॉन है। <end>\"),\n",
        "    (\"Nice to meet you.\", \"<start> आप से मिलकर अच्छा लगा। <end>\"),\n",
        "    (\"Good morning\", \"<start> शुभ प्रभात <end>\"),\n",
        "    (\"Good night\", \"<start> शुभ रात्रि <end>\"),\n",
        "    (\"Thank you\", \"<start> धन्यवाद <end>\"),\n",
        "    (\"Yes\", \"<start> हाँ <end>\"),\n",
        "    (\"No\", \"<start> नहीं <end>\")\n",
        "]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare tokenizers\n",
        "eng_tokenizer = Tokenizer(filters='')\n",
        "hin_tokenizer = Tokenizer(filters='')\n",
        "\n",
        "eng_texts = [pair[0] for pair in corpus]\n",
        "hin_texts = [pair[1] for pair in corpus]\n",
        "\n",
        "eng_tokenizer.fit_on_texts(eng_texts)\n",
        "hin_tokenizer.fit_on_texts(hin_texts)\n",
        "\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences(eng_texts)\n",
        "hin_sequences = hin_tokenizer.texts_to_sequences(hin_texts)\n",
        "\n",
        "num_encoder_tokens = len(eng_tokenizer.word_index) + 1\n",
        "num_decoder_tokens = len(hin_tokenizer.word_index) + 1\n",
        "\n",
        "max_encoder_seq_length = max([len(seq) for seq in eng_sequences])\n",
        "max_decoder_seq_length = max([len(seq) for seq in hin_sequences])\n",
        "\n",
        "latent_dim = 512\n",
        "embedding_dim = 200  # Dimension of the embedding space\n"
      ],
      "metadata": {
        "id": "wJhmV6V42dYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define and compile the training model\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(num_encoder_tokens, embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(num_decoder_tokens, embedding_dim, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ui5Ylc2J2dGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the data for the training model\n",
        "encoder_input_data = pad_sequences(eng_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
        "decoder_input_data = pad_sequences(hin_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
        "\n",
        "decoder_target_data = np.zeros((len(corpus), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "for i, seq in enumerate(hin_sequences):\n",
        "    for t, word_index in enumerate(seq):\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, word_index] = 1.0\n",
        "\n"
      ],
      "metadata": {
        "id": "MDfxLqVf2dC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Define the encoder model for inference\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Define the decoder model for inference\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n"
      ],
      "metadata": {
        "id": "B4jb56EQ2c_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# New embedding layer for inference model\n",
        "decoder_inputs_inf = Input(shape=(None,))\n",
        "decoder_embedding_inf = Embedding(num_decoder_tokens, embedding_dim, mask_zero=True)(decoder_inputs_inf)\n",
        "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(\n",
        "    decoder_embedding_inf, initial_state=decoder_states_inputs)\n",
        "decoder_states_inf = [state_h_inf, state_c_inf]\n",
        "decoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_inf] + decoder_states_inputs,\n",
        "    [decoder_outputs_inf] + decoder_states_inf\n",
        ")\n",
        "\n",
        "def preprocess_input_sentence(sentence):\n",
        "    sequence = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_encoder_seq_length, padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = hin_tokenizer.word_index['<start>']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = hin_tokenizer.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        if sampled_word == '<end>' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "def translate_sentence(sentence):\n",
        "    input_seq = preprocess_input_sentence(sentence)\n",
        "    translated_sentence = decode_sequence(input_seq)\n",
        "    return translated_sentence\n"
      ],
      "metadata": {
        "id": "reo0Dbos2c9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_sentence = \"Hello\"\n",
        "translated_sentence = translate_sentence(input_sentence)\n",
        "print(f'Input: {input_sentence}')\n",
        "print(f'Translated: {translated_sentence}')\n"
      ],
      "metadata": {
        "id": "dhZNbfs-xZYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265a4051-c754-439f-d0c5-12e6aa3e7ae1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Input: Hello\n",
            "Translated: नमस्ते\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3gsvs8F5cMyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}